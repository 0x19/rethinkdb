## Overview

### Basic primitives

Each RethinkDB process has a thread pool (see `arch/runtime/thread_pool.hpp`) with a fixed number of threads. Each of these threads is running an event loop that processes IO notifications from the OS and messages from other threads. The `arch/` directory contains primitives for timers, file and network IO, and so on in the context of the RethinkDB thread pool. The main event loop threads never run blocking IO operations; instead, there is a separate set "blocker pool" (see `arch/io/blocker_pool.hpp`) of threads used for blocking operations.

RethinkDB makes heavy use of cooperatively-scheduled coroutines; see `arch/runtime/coroutines.hpp`. The `concurrency/` directory contains many concurrency primitives designed for use with RethinkDB's coroutines. Coroutines can switch between threads by scheduling themselves to be resumed on another thread and then suspending themselves on the current thread.

Every server in the RethinkDB cluster maintains a TCP connection to every other server. The `connectivity_cluster_t` (defined in `rpc/connectivity/cluster.hpp`) is responsible for managing these connections. The `connectivity_cluster_t` exposes a low-level interface for sending untyped binary string messages between servers. The messages are guaranteed to be delivered uncorrupted and exactly once, unless the connection is lost. A common pattern is to send a message and wait for a reply, but abort the attempt if the connection is detected to have been lost; `disconnect_watcher_t` can be used to receive a notification if the connection is lost.

Most code doesn't use the `connectivity_cluster_t` directly; instead, it works with "mailboxes". A mailbox (see `rpc/mailbox/`) is a object that is created dynamically on a specific server to receive messages of a specific type. Every mailbox has an "address", which can be serialized and sent to other servers in the cluster; the other servers can use the address to send messages to the mailbox. The mailbox code uses C++ templates to make sure that the right type of message is sent, and to automatically serialize and deserialize it. RethinkDB uses a custom system based on C++ templates for serialization and deserialization; see `containers/archive/`.

Mailboxes have a chicken-and-egg problem: in order to send a message to a mailbox, one must have its address, but the only way to find out the address is to receive it in a message from a mailbox. This is resolved by using the "directory". The directory (see `rpc/directory/`) is a collection of serializable objects that each server broadcasts to the other servers it is connected to. The directory contains basic metadata about the server such as its name, as well as the addresses of mailboxes that can be used to execute users' queries, change table configurations, check the status of the server, and so on. The objects in the directory are often called "business cards", since they contain mailbox addresses.

### Server start-up process

The `main()` function is defined in `main.cc`, but it quickly transfers control to `clustering/administration/main/command_line.cc`. This file sets up the thread pool and event loop, then starts an instance of `do_serve()` (defined in `clustering/administration/main/serve.cc`) in a coroutine inside the new thread pool. `do_serve()` is responsible for setting up and tearing down all of the important components of the RethinkDB server in the right order, and passing them pointers to each other as necessary.

### Executing queries

One of the objects constructed by `do_serve()` is a `rdb_query_server_t` instance (see `rdb_protocol/query_server.hpp`), which listens for incoming TCP connections from client drivers. When it receives a message, `ql::compile_term()` (see `rdb_protocol/term.hpp`) converts the message from the client into a tree of `ql::term_t`s. The `rdb_protocol/terms/` directory contains many subclasses of `ql::term_t` for the different ReQL terms. Each subclass has an `eval()` method that does the actual work of executing the term.

The `ql::val_t` type (defined in `rdb_protocol/val.hpp`) is the basic type of a ReQL value, such as a string, table, database, etc. Simple ReQL values such as strings, arrays, objects, dates, etc. are called "datums"; they are represented by the `ql::datum_t` type (defined in `rdb_protocol/datum.hpp`).

Most queries will need to perform operations on one or more tables. An operation on a table is represented as a `read_t` or `write_t` object (defined in `rdb_protocol/protocol.hpp`); the result of the operation is represented as a `read_response_t` or `write_response_t` object. The ReQL evaluation code passes the `read_t` and `write_t` objects to the `table_query_client_t` (see `clustering/query_routing/table_query_client.hpp`). The `table_query_client_t` finds the relevant `primary_query_server_t` by looking for its business card in the directory, and sends the query to the mailbox listed on the business card. (If the query affects multiple keys, there may be multiple relevant `primary_query_server_t`s; the `read_t` or `write_t` will be broken into several sub-`read_t`s or sub-`write_t`s, which will each be sent to a different `primary_query_server_t`.) The queries pass from the `primary_query_server_t` via a series of types defined in `clustering/immediate_consistency/` until they eventually reach the `store_t`. If `read_mode` is `"outdated"`, read queries will reach the `store_t` through the `direct_query_server_t` instead of the `primary_query_server_t`.

`store_t` (defined in `rdb_protocol/store.hpp`) is responsible for executing the `read_t` and `write_t` operations on the RethinkDB storage engine and collecting the results. Every RethinkDB table is represented as a B-tree. The blocks of the B-tree are stored in RethinkDB's custom page cache, `page_cache_t` (defined in `buffer_cache/page_cache.hpp`). The `btree/` directory contains functions for executing insertion, deletion, lookup, traversal, etc. operations against the B-tree stored in the page cache. Modified pages are eventually flushed to disk via RethinkDB's log-structured serializer, `log_serializer_t` (defined in `serializer/log/log_serializer.hpp`).

When the `store_t` finishes executing the `read_t` or `write_t` against the B-tree, it returns a `read_response_t` or `write_response_t`, which is passed back via the same way the `read_t` or `write_t` arrived until it gets back to the `ql::term_t` that initiated the request. When the `eval()` method on the root term finishes executing, the response is sent back over the TCP connection to the client driver that initiated the request.

### Table configuration

The above description has mentioned several types that are involved in the process of executing queries against a table: the `store_t`, the `primary_query_server_t`, the various types defined in `clustering/immediate_consistency/`, and so on. The table's configuration (as exposed to the user in the `rethinkdb.table_config` system table) describes which servers should be performing which role for which part of the table. When the user changes the table's configuration, objects on various servers must be created and destroyed to implement the user's new configuration, but while making sure to that the tables' contents are correctly copied from server to server.

There is one `multi_table_manager_t` (defined in `clustering/table_manager/`) per server, created by `do_serve()`. It keeps a record for each table that is known to exist, whether that table has a replica on that server or not. If the server is supposed to be a replica for a given table, the `multi_table_manager_t` will construct a `table_manager_t` for that table.

RethinkDB uses the Raft consensus algorithm to manage table metadata, but not to manage user queries. RethinkDB uses a custom implementation of Raft that's designed to work with the RethinkDB event loop and networking primitives; it's implemented by the type `raft_member_t`, defined in `clustering/generic/raft_core.hpp`. There is one Raft cluster per RethinkDB table. Each `table_manager_t` instantiates a `raft_member_t`. The state managed by the Raft instance is defined as `table_raft_state_t`, defined in `clustering/table_contract/contract_metadata.hpp`. Among other things, the `table_raft_state_t` contains the table's current configuration and a collection of `contract_t` objects.

The `contract_t`s describe what each server is supposed to be doing with respect to each region of the table. The difference between the contracts and the config is that the user can change the config freely, but the contracts can only change in specific well-defined ways that ensure that the data in the tables is preserved and remains consistent. The `table_manager_t` constructs a `contract_coordinator_t` (defined in `clustering/table_contract/coordinator/`) on whichever server is the leader of the Raft cluster; it's responsible for updating the contracts to match the config. The `table_manager_t` also constructs a `contract_executor_t` (defined in `clustering/table_contract/executor/`) on every server; the `contract_executor_t` is responsible for carrying out the instructions in the contracts by constructing the `store_t`, the `primary_query_server_t`, the various types in `clustering/immediate_consistency/`, etc. The details of how this works are quite complicated; the comments in `clustering/table_contract/contract_metadata.hpp` are a good introduction.

