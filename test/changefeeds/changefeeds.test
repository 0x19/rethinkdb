#!/usr/bin/env python

import itertools, os, sys, threading, time, unittest

sys.path.append(os.path.join(os.path.dirname(__file__), os.path.pardir, 'common'))
import driver, utils

# --

lock = threading.Lock()

r = utils.import_python_driver()
dbName, tableName = utils.get_test_db_table()

# --

def synchronized(lock):
	""" Synchronization decorator """
	def wrap(f):
		def newFunction(*args, **kw):
			with lock:
				return f(*args, **kw)
		return newFunction
	return wrap

class NextWithTimeout(threading.Thread):
	
	feed = None
	result = None
	daemon = True
	
	def __init__(self, feed):
		super(NextWithTimeout, self).__init__()
		self.feed = iter(feed)
	
	@classmethod
	def getNext(cls, feed, timeout=5):
		
		instance = NextWithTimeout(feed)
		instance.start()
		deadline = time.time() + timeout
		while time.time() < deadline:
			instance.join(.1)
			if not instance.isAlive():
				if isinstance(instance.result, Exception):
					raise instance.result
				return instance.result
		# we are leaving a daemon process run
		raise Exception('Timed out waiting %d seconds for next item' % timeout)
	
	def run(self):
		try:
			return next(self.feed)
		except Exception as e:
			self.result = e

class ChangefeedsTest_Base(unittest.TestCase):
	
	# -- settings
	
	shardLevel = 2
	replicationLevel = 2
	recordsToGenerate = 1000
	samplesPerShard = 5
	
	# -- class variables
	
	cluster = None
	conn = None
	
	# --
	
	@classmethod
	def getPrimaryForShard(cls, index, table=None, db=None):
		if table is None:
			table = tableName
		if db is None:
			db = dbName
		
		serverName = r.db(dbName).table(tableName).config()['shards'].nth(index)['primary_replica'].run(cls.conn)
		for server in cls.cluster:
			if server.name == serverName:
				return server
		return None
	
	@classmethod
	def getReplicaForShard(cls, index, table=None, db=None):
		if table is None:
			table = tableName
		if db is None:
			db = dbName
		
		shardsData = r.db(dbName).table(tableName).config()['shards'].nth(index).run(cls.conn)
		replicaNames = [x for x in shardsData['replicas'] if x != shardsData['primary_replica']]
		
		for server in cls.cluster:
			if server.name in replicaNames:
				return server
		return None
	
	@classmethod
	@synchronized(lock)
	def setUp(cls):
		
		# -- short-circuit if we are already setup
		
		if cls.cluster is not None:
			return
		
		# -- start the servers
		
		cls.cluster = driver.Cluster(initial_servers=cls.shardLevel * cls.replicationLevel)
		
		server = cls.cluster[0]
		cls.conn = r.connect(host=server.host, port=server.driver_port)
		
		# -- ensure db is available
		
		if dbName not in r.db_list().run(cls.conn):
			r.db_create(dbName).run(cls.conn)
		
		# -- setup test table
		
		if tableName in r.db(dbName).table_list().run(cls.conn):
			r.db(dbName).table_drop(tableName).run(cls.conn) # ensure we have a clean table
		r.db(dbName).table_create(tableName).run(cls.conn)
		
		r.db(dbName).table(tableName).insert([{'id': x} for x in range(1, cls.recordsToGenerate + 1)]).run(cls.conn)
		
		# - shard and replicate the table
		
		primaries = iter(cls.cluster[:cls.shardLevel])
		replicas = iter(cls.cluster[cls.shardLevel:])
		
		shardPlan = []
		for primary in primaries:
			chosenReplicas = [replicas.next().name for _ in range(0, cls.replicationLevel - 1)]
			shardPlan.append({'primary_replica':primary.name, 'replicas':[primary.name] + chosenReplicas})
		assert (r.db(dbName).table(tableName).config().update({'shards':shardPlan}).run(cls.conn))['errors'] == 0
		r.db(dbName).table(tableName).wait().run(cls.conn)
	
	@classmethod
	@synchronized(lock)
	def tearDown(cls):
		
		# verify that the servers are still running
		lastError = None
		for server in cls.cluster:
			try:
				server.check()
			except Exception as e:
				lastError = e
		if lastError is not None:
			cls.cluster = None
			cls.conn = None
			raise e
	
	def makeChanges(self, samplesPerShard=None, connections=None):
		'''make a number of minor changes to records, and return those ids'''
		
		if samplesPerShard is None:
			samplesPerShard = self.samplesPerShard
		
		if connections is None:
			connections = itertools.cycle([self.conn])
		else:
			connections = itertools.cycle(connections)
		
		changedRecordIds = []
		for lower, upper in ((x[0], x[1]) for x in utils.getShardRanges(connections.next(), tableName)):
			
			conn = connections.next()
			sampleIds = (x['id'] for x in r.db(dbName).table(tableName).between(lower, upper).sample(samplesPerShard).run(conn))
			
			for thisId in sampleIds:
				r.db(dbName).table(tableName).update({'id':thisId, 'changed':True}).run(conn)
				changedRecordIds.append(thisId)
		
		changedRecordIds.sort()
		return changedRecordIds

class ChangefeedsTest_Basic(ChangefeedsTest_Base):
	'''Basic tests'''
	
	def test_simple(self):
		'''Make simple changes and ensure a single changefeed sees them'''
		
		expectedCount = self.samplesPerShard * len(utils.getShardRanges(self.conn, tableName))
		changefeed = r.db(dbName).table(tableName).changes().limit(expectedCount).run(self.conn)
		
		expectedChangedIds = self.makeChanges()
		self.assertEqual(expectedChangedIds, sorted([x['new_val']['id'] for x in changefeed]))
	
	def test_multiple_servers(self):
		'''The same changefeed on multiple servers should get the same results'''
		
		connections = [r.connect(host=x.host, port=x.driver_port) for x in self.cluster]
		expectedCount = self.samplesPerShard * len(utils.getShardRanges(connections[0], tableName))
		changefeeds = [r.db(dbName).table(tableName).changes().limit(expectedCount).run(x) for x in connections]
		
		# add data across all of the connections
		
		expectedResults = self.makeChanges()
		
		# verify that all of the feeds got the expected results
		
		for i in range(len(changefeeds)):
			feedResults = sorted([x['new_val']['id'] for x in changefeeds[i]])
			self.assertEqual(feedResults, expectedResults)

class ChangefeedsTest_Destructive(ChangefeedsTest_Base):
	'''Tests that mess with the servers'''
	
	@classmethod
	@synchronized(lock)
	def tearDown(cls):
		'''For destructive tests we close down everything after each test'''
		
		lastError = None
		for server in cls.cluster[:]:
			try:
				server.check_and_stop()
			except RuntimeError:
				pass
			except Exception as e:
				lastError = e
		cls.cluster = None
		cls.conn = None
		if lastError is not None:
			raise lastError
	
	def test_primary_falure(self):
		'''Test that we get the expected error when the primary replica for a shard fails for a range'''
		
		stable = self.getPrimaryForShard(0)
		sacrifice = self.getPrimaryForShard(1)
		self.assertTrue(stable != sacrifice, msg='There were not enough primary servers')
		
		conn = r.connect(host=stable.host, port=stable.driver_port)
		
		# start the changefeed
		
		changefeed = r.db(dbName).table(tableName).changes().run(conn)
		
		# add a change and retrieve it
					
		r.db(dbName).table(tableName).insert({}).run(conn)
		NextWithTimeout.getNext(changefeed)
		
		# kill the sacrifice server
		
		sacrifice.process.kill()
		
		# check that we error
		
		self.assertRaises(r.RqlRuntimeError, NextWithTimeout.getNext, changefeed)

	def test_secondary_failure(self):
		'''Test when a secondary shardholder fails for a range'''
		
		primary = self.getPrimaryForShard(0)
		replica = self.getReplicaForShard(0)
		
		conn = r.connect(host=primary.host, port=primary.driver_port)
		
		# ensure that the replica is not also a primary
		
		self.assertTrue(replica.name not in r.db(dbName).table(tableName).config()['shards']['primary_replica'].run(conn), msg='Replica is also a primary')
		
		# start the changefeed
		
		changefeed = r.db(dbName).table(tableName).changes().run(conn)
		
		# add a change and retrieve it
					
		r.db(dbName).table(tableName).insert({}).run(conn)
		NextWithTimeout.getNext(changefeed)
		
		# kill a secondary server
		
		replica.kill()
		
		# - add another item inside that range and make sure we still work
		
		targetRange = utils.getShardRanges(conn, table=tableName, db=dbName)[0]
		updateItem = r.db(dbName).table(tableName).between(targetRange[0], targetRange[1]).nth(0).run(conn)
		
		# with write_acks = majority
		
		self.assertTrue((r.db(dbName).table(tableName).config().update({'write_acks':'majority'}).run(conn))['errors'] == 0)
		self.assertRaises(r.RqlRuntimeError, r.db(dbName).table(tableName).get(updateItem['id']).update({'updated':True}).run, conn)
		
		# with write_acks = single
		
		self.assertTrue((r.db(dbName).table(tableName).config().update({'write_acks':'single'}).run(conn))['errors'] == 0)
		r.RqlRuntimeError, r.db(dbName).table(tableName).get(updateItem['id']).update({'updated':True}).run(conn)
		NextWithTimeout.getNext(changefeed)

	def test_connection_death(self):
		'''Test that the client handles the death of the server at the other end of the connection correctly'''
		
		stable = self.getPrimaryForShard(0)
		sacrifice = self.getPrimaryForShard(1)
		
		stable_conn = r.connect(host=stable.host, port=stable.driver_port)
		sacrifice_conn = r.connect(host=sacrifice.host, port=sacrifice.driver_port)
		
		# start the changefeed
		
		changefeed = r.db(dbName).table(tableName).changes().run(sacrifice_conn)
		
		# add a change and retrieve it
					
		r.db(dbName).table(tableName).insert({}).run(stable_conn)
		NextWithTimeout.getNext(changefeed)
		
		# kill a primary server
		
		sacrifice.process.kill()
		
		# change an item in the stable range
		
		stableRange = utils.getShardRanges(stable_conn, table=tableName, db=dbName)[0]
		updateItem = r.db(dbName).table(tableName).between(stableRange[0], stableRange[1]).nth(0).run(stable_conn)
		r.db(dbName).table(tableName).get(updateItem['id']).update({'updated':True}).run(stable_conn)
		
		# check that we error
		
		self.assertRaises(r.RqlDriverError, NextWithTimeout.getNext, changefeed)

# ===== main

def main():
	unittest.main()

if __name__ == '__main__':
	main()
